
@article{gebruDatasheetsDatasets2020,
  title = {Datasheets for {{Datasets}}},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'e} III, Hal and Crawford, Kate},
  year = {2020},
  month = jan,
  abstract = {The machine learning community currently has no standardized process for documenting datasets. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
  archivePrefix = {arXiv},
  eprint = {1803.09010},
  eprinttype = {arxiv},
  file = {/Users/audrey/Zotero/storage/AUYFW6Q4/Gebru et al. - 2020 - Datasheets for Datasets.pdf},
  journal = {arXiv:1803.09010 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}



@article{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  archivePrefix = {arXiv},
  eprint = {1906.02243},
  eprinttype = {arxiv},
  file = {/Users/audrey/Zotero/storage/9RZIGZB2/Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf},
  journal = {arXiv:1906.02243 [cs]},
  keywords = {carbon,climate change,Computer Science - Computation and Language,energy,environment,nlp},
  language = {en},
  primaryClass = {cs}
}


